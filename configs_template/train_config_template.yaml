config_type: RunConfig

########## Wandb init parameters ##########
### Wandb parameters of the init function as a dict of kwargs that will be deserialized with **. It should 
### follow the wandb nomenclature.
# Optional[dict[str, Any]] - Default to {}.
wandb_init_params:
    project: train
    name: train-0
    id: train-0
    resume: allow


tasks:

    train_task:
        task_type: train
        inputs: dataset:inputs_directory

        config:
            ###############################################
            ########## Initialize fct parameters ##########
            ###############################################

            ########## Seed ##########
            ### Seed used to generate random number. If fixed, it makes results reproducible.
            # Optional[int | float] - Default to None.
            manual_seed: null

            ########## Dataloader parameters ##########
            ### Images extension to search for in the dataset root directory.
            # Optional[str] - Default to '.png'.
            images_extension: .png
            ### Batch size for the training.
            # int
            batch_size: 2

            ########## Optimizer parameters ##########
            ### Optimizer to use during training. It should follow the torch nomenclature. Choose 'Adam'.
            # Optional[Literal("Adam")] - Default to 'Adam'.
            optimizer: Adam
            ### Optimizer parameters passed as a dict of kwargs that will be deserialized with **. It should follow the
            ### PyTorch nomenclature.
            # Optional[dict[str, Any]] - Default to {'lr': 1e-5}.
            optimizer_params:
                lr: 0.00001

            ########## LR scheduler parameters ##########
            ### Learning rate scheduler type. It should follow the PyTorch nomenclature. Choose between: 
            ### 'ReduceLROnPlateau' and None.
            # Optional[Literal['ReduceLROnPlateau'] | None] - Default to None.
            lr_scheduler: null
            ### Learning rate scheduler parameters as dict of kwargs that will be deserialized with **. It should 
            ### follow the PyTorch nomenclature.
            # Optional[dict[str, Any]] - Default to {}.
            lr_scheduler_params: null


            ##########################################
            ########## Train fct parameters ##########
            ##########################################

            ########## Training parameters ##########
            ### Number of epochs on which to train the model.
            # int
            n_epochs: 1
            ### The reduction factor by which focal_loss_weight or regression_loss_weight will be multiplied. 
            ### Ex: loss_reduction_factor=4 => w_focal_loss=1 and w_reg_loss=1/4 or w_focal_loss=1/4 and w_reg_loss=1.
            # Optional[Annotated[float, Field(ge=0.1, le=10)]] - Default to 1.0.
            loss_reduction_factor: 1.0
            ### Sign indicating which loss will be reduced over the other: -1 indicates the reduction of the focal_loss,
            ### while 1 indicates the reduction of the regression_loss.
            ### Ex: loss_reduction_factor=4 and loss_reduction_sign_indicator=-1 => w_focal_loss=1/4 and w_reg_loss=1
            # Optional[Literal[-1, 1]] - Default to 1.
            loss_reduction_sign_indicator: 1

            ########## Matching parameters ##########
            ### The iou value from which we consider that the detection matches the GT bbox.
            # Optional[float] - Default to 0.1.
            matching_iou_threshold: 0.1

            ########## Wandb logging parameter ##########
            ### Whether to log model_checkpoint to wandb or not.
            # Optional[bool] - Default to True.
            wandb_log_model_checkpoint: True

            ########## Saving parameters ##########
            ### The way the model is saved wheter the whole model, whether only the weights or both of them.
            # Optional[Literal['model', 'model_state_dict', 'both']] - Default to 'model_state_dict'.
            model_saving_type: model
            ### Whether to save the optimizer_sate_dict in the checkpoint or not.
            # Optional[bool] - Default to True.
            save_optimizer: True
            ### The frequency at which to save the model weights. -1 means no saving.
            # Optional[int] - Default to -1.
            saving_frequency: 1
            ### The metric used to evaluate the model's checkpoints to identify the best one to save.
            # Optional[str | None] - Default to "val.loss"
            checkpoint_scoring_metric: val.loss
            ### Indicates whether the metric used to evaluate the model's checkpoints should be maximized or minimized.
            # Optional[Literal["min", "max"] | None] - Default to "min"
            checkpoint_scoring_order: min
            ### The file name to give to the model backups.
            # Optional[str] - Default  to 'model'.
            checkpoint_file_name: retinanet

            ########## Tqdm ##########
            ### Parameter to disable the display of the epoch progress bar
            # Optional[bool | None] - Default to None.
            disable_epoch_tqdm: null
            ### Parameter to disable the display of the batch progress bar
            # Optional[bool | None] - Default to None.
            disable_batch_tqdm: null
